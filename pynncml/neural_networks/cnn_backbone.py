import torch
import torch.nn as nn
from pynncml import neural_networks
from pynncml.neural_networks.normalization import InputNormalization


class CNNBackbone(nn.Module):
    """
    CNN-based backbone with GAN-style metadata fusion for rain estimation.
    
    This module processes 1D temporal sequences using convolutional layers and fuses 
    metadata information in a GAN-style manner (concatenation and projection).
    
    :param normalization_cfg: InputNormalizationConfig which holds the normalization parameters.
    :param input_size: int that represents the input sequence length.
    :param n_filters: int that represents the number of filters in convolutional layers.
    :param kernel_size: int that represents the kernel size for convolutions.
    :param metadata_input_size: int that represents the metadata input size.
    :param metadata_n_features: int that represents the metadata feature size.
    :param n_layers: int that represents the number of CNN layers.
    :param dropout_rate: float that represents the dropout rate.
    """

    def __init__(self, 
                 normalization_cfg: neural_networks.InputNormalizationConfig,
                 input_size: int,
                 n_filters: int = 128,
                 kernel_size: int = 3,
                 metadata_input_size: int = 2,
                 metadata_n_features: int = 32,
                 n_layers: int = 3,
                 dropout_rate: float = 0.1):

        super(CNNBackbone, self).__init__()
        
        self.input_size = input_size
        self.n_filters = n_filters
        self.metadata_n_features = metadata_n_features
        self.n_layers = n_layers
        
        # Normalization layer
        self.normalization = InputNormalization(normalization_cfg)
        
        # Metadata processing (GAN-style)
        self.fc_meta = nn.Sequential(
            nn.Linear(metadata_input_size, metadata_n_features),
            nn.ReLU(),
            nn.Linear(metadata_n_features, metadata_n_features),
            nn.ReLU()
        )
        
        # 1D CNN layers
        cnn_layers = []
        
        # First layer: from 1 channel to n_filters
        cnn_layers.append(nn.Conv1d(1, n_filters, kernel_size=kernel_size, padding=kernel_size//2))
        cnn_layers.append(nn.BatchNorm1d(n_filters))
        cnn_layers.append(nn.ReLU())
        cnn_layers.append(nn.Dropout(dropout_rate))
        
        # Additional layers
        for i in range(n_layers - 1):
            cnn_layers.append(nn.Conv1d(n_filters, n_filters, kernel_size=kernel_size, padding=kernel_size//2))
            cnn_layers.append(nn.BatchNorm1d(n_filters))
            cnn_layers.append(nn.ReLU())
            cnn_layers.append(nn.Dropout(dropout_rate))
        
        self.cnn_layers = nn.Sequential(*cnn_layers)
        
        # Metadata fusion layers (GAN-style conditioning)
        self.meta_projection = nn.Linear(metadata_n_features, n_filters)
        
        # Final processing layer
        self.final_conv = nn.Conv1d(n_filters, n_filters, kernel_size=1)
        
    def total_n_features(self) -> int:
        """
        Return the total number of features generated by the backbone.
        
        :return: integer number stating the total number of features.
        """
        return self.n_filters

    def forward(self, data: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the CNN backbone.
        
        :param data: A tensor of shape [batch_size, num_samples] containing the 1D temporal sequence.
        :param metadata: A tensor of shape [batch_size, metadata_features] containing metadata.
        :return: A tensor of shape [batch_size, num_samples, n_filters] containing processed features.
        """
        batch_size, num_samples = data.shape
        
        # Expand data to match normalization expectations: [batch_size, num_samples, 1]
        data_expanded = data.unsqueeze(-1)
        
        # Normalize inputs
        input_tensor, input_meta_tensor = self.normalization(data_expanded, metadata)
        
        # Process metadata (GAN-style)
        meta_features = self.fc_meta(input_meta_tensor)  # [batch_size, metadata_n_features]
        
        # Reshape for 1D convolution: [batch_size, 1, num_samples]
        x = input_tensor.squeeze(-1).unsqueeze(1)
        
        # Apply CNN layers
        x = self.cnn_layers(x)  # [batch_size, n_filters, num_samples]
        
        # GAN-style metadata fusion
        # Project metadata to feature space
        meta_projected = self.meta_projection(meta_features)  # [batch_size, n_filters]
        meta_projected = meta_projected.unsqueeze(-1)  # [batch_size, n_filters, 1]
        
        # Broadcast and add metadata features (conditioning)
        meta_broadcasted = meta_projected.expand(-1, -1, num_samples)  # [batch_size, n_filters, num_samples]
        x = x + meta_broadcasted
        
        # Final processing
        x = self.final_conv(x)  # [batch_size, n_filters, num_samples]
        
        # Reshape to [batch_size, num_samples, n_filters] for compatibility
        output = x.transpose(1, 2)
        
        return output


class CNNRainEstimator(nn.Module):
    """
    Complete CNN-based rain estimator with GAN-style metadata fusion.
    
    :param normalization_cfg: InputNormalizationConfig for input normalization.
    :param input_size: int that represents the input sequence length.
    :param n_filters: int that represents the number of CNN filters.
    :param kernel_size: int that represents the kernel size for convolutions.
    :param metadata_input_size: int that represents the metadata input size.
    :param metadata_n_features: int that represents the metadata feature size.
    :param n_layers: int that represents the number of CNN layers.
    :param dropout_rate: float that represents the dropout rate.
    """
    
    def __init__(self,
                 normalization_cfg: neural_networks.InputNormalizationConfig,
                 input_size: int,
                 n_filters: int = 128,
                 kernel_size: int = 3,
                 metadata_input_size: int = 2,
                 metadata_n_features: int = 32,
                 n_layers: int = 3,
                 dropout_rate: float = 0.1):
        
        super(CNNRainEstimator, self).__init__()
        
        # CNN backbone
        self.backbone = CNNBackbone(
            normalization_cfg=normalization_cfg,
            input_size=input_size,
            n_filters=n_filters,
            kernel_size=kernel_size,
            metadata_input_size=metadata_input_size,
            metadata_n_features=metadata_n_features,
            n_layers=n_layers,
            dropout_rate=dropout_rate
        )
        
        # Output head for rain estimation
        self.rain_head = nn.Sequential(
            nn.Linear(n_filters, n_filters // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(n_filters // 2, 1)
        )
        
    def forward(self, data: torch.Tensor, metadata: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the CNN rain estimator.
        
        :param data: A tensor of shape [batch_size, num_samples] containing the 1D temporal sequence.
        :param metadata: A tensor of shape [batch_size, metadata_features] containing metadata.
        :return: A tensor of shape [batch_size, num_samples] containing rain rate estimates.
        """
        # Get features from backbone
        features = self.backbone(data, metadata)  # [batch_size, num_samples, n_filters]
        
        # Apply rain estimation head
        rain_output = self.rain_head(features)  # [batch_size, num_samples, 1]
        
        # Squeeze to get [batch_size, num_samples]
        return rain_output.squeeze(-1) 